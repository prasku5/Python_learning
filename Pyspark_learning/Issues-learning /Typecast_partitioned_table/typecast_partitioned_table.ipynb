{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set SPARK_HOME and JAVA_HOME environment variables\n",
    "os.environ['SPARK_HOME'] = '/usr/local/Cellar/apache-spark/3.5.1/libexec'\n",
    "os.environ['JAVA_HOME'] = '/usr/local/opt/openjdk/libexec/openjdk.jdk/Contents/Home'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Spark Error in Real-Time Project\n",
    "\n",
    "### Task:\n",
    "\n",
    "Task: Change the data type of the column processed_datetime from DATE to TIMESTAMP\n",
    "\n",
    "Error: \n",
    "error reading the parquet file at s3://big-data-project/test-schema/test-table/load_ts=20240731160000\n",
    "column \"processed_datetime\" expected TIMESTAMP but found INT4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Context\n",
    "\n",
    "The error arises when the data type in the Parquet files does not match the expected schema in the table. In this case, the `processed_datetime` column is expected to be a `TIMESTAMP`, but the data in the partitions is still in `DATE` format. This mismatch often occurs because of changes in schema definitions in partitioned tables.\n",
    "\n",
    "\n",
    "### Steps to Rectify the Error\n",
    "\n",
    "1. **Identify the Mismatched Partitions:**\n",
    "   - Check which partitions are using the old data type. In the blog post context, this would mean identifying which partitions still have the `DATE` type instead of the `TIMESTAMP`.\n",
    "\n",
    "2. **Update the Schema of Existing Partitions:**\n",
    "   - Alter the data type of columns in the partitions. This could involve manually altering the schema or rewriting the data.\n",
    "\n",
    "3. **Drop the Old Partitions (if applicable):**\n",
    "   - If partitions are not needed or can be reprocessed, drop them. This is often easier in sandbox or development environments.\n",
    "\n",
    "4. **Reprocess the Data (if needed):**\n",
    "   - Delete the data in S3 and reprocess it from the source. This ensures that all data follows the new schema and eliminates schema mismatch issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "| data|processed_datetime|\n",
      "+-----+------------------+\n",
      "|data2|        2023-07-30|\n",
      "|data1|        2023-07-31|\n",
      "|data3|        2023-07-29|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, to_timestamp\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SparkDataTypeChange\").getOrCreate()\n",
    "\n",
    "# Sample data with DATE type column\n",
    "data = [(\"2023-07-31\", \"data1\"), (\"2023-07-30\", \"data2\"), (\"2023-07-29\", \"data3\")]\n",
    "schema = [\"processed_datetime\", \"data\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df = df.withColumn(\"processed_datetime\", to_date(col(\"processed_datetime\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Save DataFrame as Parquet with partitioning\n",
    "df.write.mode(\"overwrite\").partitionBy(\"processed_datetime\").parquet(\"Pyspark_learning/Issues-learning /Typecast_partitioned_table/data/processed_date\")\n",
    "\n",
    "# Read back the Parquet files to simulate the original scenario\n",
    "df_loaded = spark.read.parquet(\"Pyspark_learning/Issues-learning /Typecast_partitioned_table/data/processed_date\")\n",
    "df_loaded.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's handle the error by modifying the partitions and updating the data type.\n",
    "\n",
    "Option 1: Alter the data type of columns in the partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+\n",
      "| data| processed_datetime|\n",
      "+-----+-------------------+\n",
      "|data3|2023-07-29 00:00:00|\n",
      "|data1|2023-07-31 00:00:00|\n",
      "|data2|2023-07-30 00:00:00|\n",
      "+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Altering the schema to TIMESTAMP (simulated)\n",
    "# Recreate DataFrame with TIMESTAMP type\n",
    "df_timestamp = df.withColumn(\"processed_datetime\", to_timestamp(col(\"processed_datetime\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Overwrite the existing partition with the new TIMESTAMP type\n",
    "df_timestamp.write.mode(\"overwrite\").partitionBy(\"processed_datetime\").parquet(\"Pyspark_learning/Issues-learning /Typecast_partitioned_table/data/processed_date\")\n",
    "\n",
    "# Read the updated Parquet files to verify\n",
    "df_loaded_updated = spark.read.parquet(\"Pyspark_learning/Issues-learning /Typecast_partitioned_table/data/processed_date\")\n",
    "df_loaded_updated.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Partitions\n",
    "\n",
    "To drop partitions, you can use a SQL command to alter the table and drop specific partitions. This approach assumes you're working with a managed table in Spark SQL. If using DataFrames directly, you may need to delete the relevant files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DropPartitionsExample\").getOrCreate()\n",
    "\n",
    "# Assuming you have a managed table\n",
    "table_name = \"your_table_name\"\n",
    "partition_column = \"processed_datetime\"\n",
    "partition_value_to_drop = \"2023-07-31\"  # Example partition value\n",
    "\n",
    "# Drop specific partition\n",
    "spark.sql(f\"ALTER TABLE {table_name} DROP IF EXISTS PARTITION ({partition_column} = '{partition_value_to_drop}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting and Reprocessing Data\n",
    "\n",
    "\n",
    "To delete data from S3 and reprocess it, you can use Spark to delete the old data and then write new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, to_timestamp\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DeleteAndReprocessData\").getOrCreate()\n",
    "\n",
    "# Path to the old data\n",
    "old_data_path = \"s3://your-bucket/path/to/old/data\"\n",
    "\n",
    "# Delete old data\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Local file system simulation (adjust for S3 using boto3 or other methods)\n",
    "shutil.rmtree(old_data_path, ignore_errors=True)\n",
    "\n",
    "# Sample data with new TIMESTAMP type column\n",
    "data = [(\"2023-07-31 00:00:00\", \"data1\"), (\"2023-07-30 00:00:00\", \"data2\"), (\"2023-07-29 00:00:00\", \"data3\")]\n",
    "schema = [\"processed_datetime\", \"data\"]\n",
    "\n",
    "# Create DataFrame with TIMESTAMP type\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df = df.withColumn(\"processed_datetime\", to_timestamp(col(\"processed_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Save DataFrame as Parquet with partitioning\n",
    "df.write.mode(\"overwrite\").partitionBy(\"processed_datetime\").parquet(\"s3://your-bucket/path/to/new/data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema Evolution\n",
    "Schema evolution is a feature of some data storage formats and platforms, like Delta Lake or Apache Hudi.\n",
    "For standard Parquet files and Spark, you might not have direct schema evolution support, but you can manually handle schema updates.\n",
    "\n",
    "Using Delta Lake (an example for schema evolution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SchemaEvolutionExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to Delta table\n",
    "delta_table_path = \"s3://your-bucket/path/to/delta_table\"\n",
    "\n",
    "# Convert to Delta format (if not already)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "\n",
    "# Read Delta table\n",
    "delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "\n",
    "# Alter the schema (add new column example)\n",
    "delta_table.alterTableAddColumn(\"new_column STRING\")\n",
    "\n",
    "# Upsert data with new schema\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "new_data = [( \"2023-08-01 00:00:00\", \"data4\", \"new_value\")]\n",
    "new_df = spark.createDataFrame(new_data, [\"processed_datetime\", \"data\", \"new_column\"])\n",
    "\n",
    "new_df.write.format(\"delta\").mode(\"append\").save(delta_table_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
